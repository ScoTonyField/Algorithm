## Machine Learning_KNN

#### 1-基本介绍

KNN算法是一种基本的分类和回归算法，在KNN算法过程中，三大基本要素为K值选择、距离度量、分类规则。KNN是一种分类(classification)算法，它输入基于实例的学习（instance-based learning），属于懒惰学习（lazy learning）即KNN没有显式的学习过程，也就是说没有训练阶段，数据集事先已有了分类和特征值，待收到新样本后直接进行处理。

#### 2-算法

- 计算测试数据与各个训练数据之间的距离；
- 按照距离的递增关系进行排序；
- 选取距离最小的K个点(奇数)；
- 确定前K个点所在类别的出现频率；
- 返回前K个点中出现频率最高的类别作为测试数据的预测分类

#### 3-总结

- KNN算法是最简单有效的分类算法，简单且容易实现。当训练数据集很大时，需要大量的存储空间，而且需要计算待测样本和训练数据集中所有样本的距离，所以非常耗时
- KNN对于随机分布的数据集分类效果较差，对于类内间距小，类间间距大的数据集分类效果好，而且对于边界不规则的数据效果好于线性分类器。
- KNN对于样本不均衡的数据效果不好，需要进行改进。改进的方法时对k个近邻数据赋予权重，比如距离测试样本越近，权重越大。
- KNN很耗时，时间复杂度为O(n)，一般适用于样本数较少的数据集，当数据量大时，可以将数据以树的形式呈现，能提高速度，常用的有kd-tree和ball-tree。

#### 4-KD Tree

**实现步骤：**

- 选择一个维度（x，y，z ......）
- 选出这些点这个维度值的中位数
- 将数据按中位数分为两部分
- 对这两部分数据同样执行上述操作，直到数据点的数目为 1（递归）

**实现样例：**

![img](https://upload-images.jianshu.io/upload_images/15548795-1550767e12caa5cc.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

**总结：**

- 暴力搜索在大样本数N中表现的显著改善
- 近邻搜索的计算成本可以降低为O[DNlog(N)]或更低
- KD 树的构造非常快，对于低维度 (D<20) 近邻搜索也非常快, 当D增长到很大时，效率变低—— 这就是所谓的 “维度灾难” 的一种体现

